{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57539a60-6694-46cc-a5d9-e6e485ccbf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GitProcessor, GitForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b81f98-28e4-4133-a01d-70389aa68f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afa8c7a-8fce-4196-aa10-ff72042456cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "git_processor = GitProcessor.from_pretrained(\"microsoft/git-large\")\n",
    "git_model = GitForCausalLM.from_pretrained(\"microsoft/git-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e88357-1706-4b11-bba6-7933baa0e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4dfbf0-33de-4ee1-95da-bdb1830f2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7392af-55d2-43ee-959b-ec86ee70244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(text):\n",
    "    return text.replace(\"[ unused0 ]\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828bf6d3-a6c2-4768-8be6-7fab2f71fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_captions(base_caption, desc1, desc2):\n",
    "    prompt = f\"\"\"\n",
    "Given the base caption that is true and factual:\n",
    "\\\"{base_caption}\\\"\n",
    "\n",
    "And two descriptive captions:\n",
    "1) {desc1}\n",
    "2) {desc2}\n",
    "\n",
    "Write a short, coherent description that is faithful to the base caption but incorporates descriptive elements from captions 1 and 2 without contradicting the original meaning.\n",
    "\"\"\"\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(llama_model.device)\n",
    "    with torch.no_grad():\n",
    "        ids = llama_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "        text = llama_tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "        result = text[len(prompt):].strip()\n",
    "        for prefix in [\"Example:\", \"example:\"]:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234053d3-a66b-40f0-9fa2-8a0d2b912c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SAVE_EVERY = 500\n",
    "MAX_IMAGES = 5000\n",
    "OUTPUT_FILE =\"wikiart_captions_embeddings_10000_14999.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78173ea8-c933-4ad0-af77-a385f1442bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch_to_jsonl(batch_data, file_path):\n",
    "    with open(file_path, \"a\") as f:\n",
    "        for item in batch_data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf3ceac1-04e4-4dc1-a0b3-efb3b23a0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_examples):\n",
    "    results = []\n",
    "    images = []\n",
    "    image_ids = []\n",
    "\n",
    "    for ex in batch_examples:\n",
    "        img = ex[\"image\"].convert(\"RGB\")\n",
    "        images.append(img)\n",
    "        image_ids.append(ex[\"image\"].filename if hasattr(ex[\"image\"], \"filename\") else None)\n",
    "\n",
    "\n",
    "    if not images:\n",
    "        return results\n",
    "\n",
    "    pixel_values = git_processor(images=images, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "    # do_sample=False\n",
    "    with torch.no_grad():\n",
    "        base_ids = git_model.generate(pixel_values=pixel_values, max_new_tokens=30, do_sample=False)\n",
    "    base_captions = [clean_caption(git_processor.tokenizer.decode(ids, skip_special_tokens=True)) for ids in base_ids]\n",
    "\n",
    "    # do_sample=True\n",
    "    with torch.no_grad():\n",
    "        sampled_ids = git_model.generate(\n",
    "            pixel_values=pixel_values,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            top_k=100,\n",
    "            temperature=0.8,\n",
    "            num_return_sequences=2\n",
    "        )\n",
    "        \n",
    "    sampled_ids = sampled_ids.view(len(images), 2, -1)\n",
    "\n",
    "    merged_captions = []\n",
    "    for i in range(len(images)):\n",
    "        desc1 = clean_caption(git_processor.tokenizer.decode(sampled_ids[i][0], skip_special_tokens=True))\n",
    "        desc2 = clean_caption(git_processor.tokenizer.decode(sampled_ids[i][1], skip_special_tokens=True))\n",
    "        merged = merge_captions(base_captions[i], desc1, desc2)\n",
    "        merged_captions.append(merged)\n",
    "\n",
    "    # Эмбеддинги с sentence-transformers\n",
    "    embeddings = sbert_model.encode(merged_captions, convert_to_numpy=True).tolist()\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        results.append({\n",
    "            \"image_id\": image_ids[i],\n",
    "            \"caption\": merged_captions[i],\n",
    "            \"embedding\": embeddings[i]\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da29a8d9-bcb0-4e93-adb9-3191524d4df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:38<?, ?it/s]\n",
      "100%|██████████████████████████████████| 10000/10000 [17:26:00<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово! Сохранено 10000 примеров в wikiart_captions_embeddings.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "\n",
    "# processed_count = 0\n",
    "# buffer = []\n",
    "\n",
    "# pbar = tqdm(total=MAX_IMAGES)\n",
    "# batch = []\n",
    "\n",
    "# for example in dataset:\n",
    "#     if processed_count >= MAX_IMAGES:\n",
    "#         break\n",
    "#     batch.append(example)\n",
    "#     if len(batch) == BATCH_SIZE:\n",
    "#         result_batch = process_batch(batch)\n",
    "#         buffer.extend(result_batch)\n",
    "#         processed_count += len(result_batch)\n",
    "#         pbar.update(len(result_batch))\n",
    "#         batch = []\n",
    "\n",
    "#         if processed_count % SAVE_EVERY == 0:\n",
    "#             save_batch_to_jsonl(buffer, OUTPUT_FILE)\n",
    "#             buffer = []\n",
    "\n",
    "# # Сохраняем остатки\n",
    "# if buffer:\n",
    "#     save_batch_to_jsonl(buffer, OUTPUT_FILE)\n",
    "\n",
    "# pbar.close()\n",
    "# print(f\"Готово! Сохранено {processed_count} примеров в {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf2406-8b7c-4edd-b3ec-c48d375d8b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "SAVE_EVERY = 500\n",
    "MAX_IMAGES = 5000  \n",
    "START_INDEX = 10000  \n",
    "OUTPUT_FILE = \"wikiart_captions_embeddings_10000_14999.jsonl\" \n",
    "\n",
    "dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "\n",
    "processed_count = 0\n",
    "buffer = []\n",
    "\n",
    "pbar = tqdm(total=MAX_IMAGES)\n",
    "batch = []\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i < START_INDEX:\n",
    "        continue\n",
    "        \n",
    "    if processed_count >= MAX_IMAGES:\n",
    "        break\n",
    "        \n",
    "    batch.append(example)\n",
    "    if len(batch) == BATCH_SIZE:\n",
    "        result_batch = process_batch(batch)\n",
    "        buffer.extend(result_batch)\n",
    "        processed_count += len(result_batch)\n",
    "        pbar.update(len(result_batch))\n",
    "        batch = []\n",
    "\n",
    "        if processed_count % SAVE_EVERY == 0:\n",
    "            save_batch_to_jsonl(buffer, OUTPUT_FILE)\n",
    "            buffer = []\n",
    "\n",
    "if buffer:\n",
    "    save_batch_to_jsonl(buffer, OUTPUT_FILE)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"Готово! Сохранено {processed_count} примеров в {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab72c7-a19c-4d35-b054-3eb576d9295e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
