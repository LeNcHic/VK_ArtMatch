{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50903414-2f37-4944-911e-f34377475eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "from transformers import (\n",
    "    GitProcessor,\n",
    "    GitForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a7afa8-a33e-41cd-bc2e-ee4ebfc7f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6416916-4261-4966-bbeb-05f8cec9655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "git_processor = GitProcessor.from_pretrained(\"microsoft/git-large\")\n",
    "git_model = GitForCausalLM.from_pretrained(\"microsoft/git-large\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58806414-8965-490e-b532-6728b7850f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8a1e0a-c123-4af7-952a-113cfc90c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddd1484-78b6-465e-9404-a68b127e1500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df949bd1-1559-4bb5-b21a-04545101ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiart_dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "\n",
    "with open(\"../join_jsons/wikiart_10000_combined.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03328949-ab5e-4a78-af50-94c064d7818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = faiss.read_index(\"../create_index/image_index_llama.faiss\")\n",
    "text_index = faiss.read_index(\"../create_index/text_index_llama.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bbf219b-711e-4da5-93d1-cc83fd09b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(text):\n",
    "    return text.replace(\"[ unused0 ]\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40858706-afb5-42f5-b094-b26b6b964e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(image: Image.Image):\n",
    "    inputs = git_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "    captions = []\n",
    "    with torch.no_grad():\n",
    "        deterministic_ids = git_model.generate(\n",
    "            pixel_values=inputs,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=False\n",
    "        )\n",
    "        captions.append(clean_caption(git_processor.tokenizer.decode(deterministic_ids[0], skip_special_tokens=True)))\n",
    "\n",
    "        sampled_ids = git_model.generate(\n",
    "            pixel_values=inputs,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            top_k=100,\n",
    "            temperature=0.8,\n",
    "            num_return_sequences=2\n",
    "        )\n",
    "        sampled = git_processor.tokenizer.batch_decode(sampled_ids, skip_special_tokens=True)\n",
    "        captions.extend([clean_caption(c) for c in sampled])\n",
    "\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5a385f-8557-45eb-ab7e-b3cb7a8e7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_caption(base, desc1, desc2):\n",
    "    prompt = f\"\"\"\n",
    "Given the base caption that is true and factual:\n",
    "\\\"{base}\\\"\n",
    "\n",
    "And two descriptive captions:\n",
    "1) {desc1}\n",
    "2) {desc2}\n",
    "\n",
    "Write a short, coherent description that is faithful to the base caption but incorporates descriptive elements from captions 1 and 2 without contradicting the original meaning.\n",
    "\"\"\"\n",
    "    inputs = tokenizer_llama(prompt, return_tensors=\"pt\").to(model_llama.device)\n",
    "    with torch.no_grad():\n",
    "        output = model_llama.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "        text = tokenizer_llama.decode(output[0], skip_special_tokens=True)\n",
    "        answer = text[len(prompt):].strip()\n",
    "        for prefix in [\"Example:\", \"example:\"]:\n",
    "            if answer.startswith(prefix):\n",
    "                answer = answer[len(prefix):].strip()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9473e95a-0471-4665-a128-dde7fe398c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text):\n",
    "    emb = text_encoder.encode([text], normalize_embeddings=False).astype(\"float32\")\n",
    "    faiss.normalize_L2(emb)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0274ce6-5855-4ae1-bad2-64b3184241af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    emb = image_features.cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(emb)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2cd3f3a-a065-45f8-9f9d-d477a02ef1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_with_images(embedding, index, top_k=5):\n",
    "    D, I = index.search(embedding, top_k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        try:\n",
    "            idx_int = int(idx)\n",
    "            item = wikiart_dataset[idx_int]\n",
    "            img = item[\"image\"]\n",
    "            caption = f\"ID: {idx_int}\"\n",
    "            results.append((img, caption))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0455036b-bad6-4434-b391-f6aba56e5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_images(image: Image.Image):\n",
    "    captions = generate_captions(image)\n",
    "    refined = refine_caption(captions[0], captions[1], captions[2])\n",
    "\n",
    "    text_emb = get_text_embedding(refined)\n",
    "    image_emb = get_image_embedding(image)\n",
    "\n",
    "    text_results = get_results_with_images(text_emb, text_index)\n",
    "    image_results = get_results_with_images(image_emb, image_index)\n",
    "\n",
    "    return refined, text_results, image_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d12d2ae3-0f6f-4a73-8477-9d367fa9807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/gradio/utils.py\", line 894, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/mx/jk4d5_vn55j2l9vchtq66fqw0000gn/T/ipykernel_31155/4180313397.py\", line 20, in wrapper\n",
      "    caption, text_results, image_results = search_similar_images(image)\n",
      "                                           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/var/folders/mx/jk4d5_vn55j2l9vchtq66fqw0000gn/T/ipykernel_31155/1688158666.py\", line 2, in search_similar_images\n",
      "    captions = generate_captions(image)\n",
      "  File \"/var/folders/mx/jk4d5_vn55j2l9vchtq66fqw0000gn/T/ipykernel_31155/4185784357.py\", line 2, in generate_captions\n",
      "    inputs = git_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
      "             ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/transformers/models/git/processing_git.py\", line 99, in __call__\n",
      "    raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n",
      "ValueError: You have to specify either text or images. Both cannot be none.\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(title=\"üé® Semantic WikiArt Search (BLIP + CLIP)\") as demo:\n",
    "    gr.Markdown(\"## Semantic WikiArt Search\\n–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –Ω–∞–π–¥–∏—Ç–µ –ø–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\")\n",
    "\n",
    "    input_image = gr.Image(label=\"üì• –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"pil\")\n",
    "\n",
    "    caption_output = gr.Textbox(label=\"üìú –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\")\n",
    "\n",
    "    gr.Markdown(\"### üîç –ü–æ—Ö–æ–∂–∏–µ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (—Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)\")\n",
    "    text_gallery = gr.Gallery(columns=5, label=\"–ü–æ –æ–ø–∏—Å–∞–Ω–∏—é\", height=\"auto\")\n",
    "\n",
    "    gr.Markdown(\"### üé® –ü–æ—Ö–æ–∂–∏–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (–≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)\")\n",
    "    image_gallery = gr.Gallery(columns=5, label=\"–ü–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é\", height=\"auto\")\n",
    "\n",
    "    def wrapper(image):\n",
    "        caption, text_results, image_results = search_similar_images(image)\n",
    "        return caption, text_results, image_results\n",
    "\n",
    "    input_image.change(\n",
    "        fn=wrapper,\n",
    "        inputs=input_image,\n",
    "        outputs=[caption_output, text_gallery, image_gallery]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e01ea85a-5a8f-4bc7-a937-8306762439e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://619f08b350bae78de8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://619f08b350bae78de8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d87588-7129-4330-843a-704a13402a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
